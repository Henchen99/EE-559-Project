{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 559 Project\n",
    "Ronald Huang & Henry Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as datetime\n",
    "#import seaborn as sns\n",
    "from collections import Counter, deque\n",
    "\n",
    "\n",
    "# For Computing Priors\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, classification_report,confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "# For Visualisation\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "# For Model Selection\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.exceptions import ConvergenceWarning, UndefinedMetricWarning\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# For creating Tensorflow models\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from keras.layers import Dense, Input, Dropout, SimpleRNN, GRU, LSTM, Conv1D\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# For plotting ROC and Precision Recall curves\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "\n",
    "# For OS agnostic path handling\n",
    "from os import path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = np.loadtxt('data/FLIR_groups1and2_train.csv', delimiter = ',', skiprows = 2)\n",
    "# train = pd.read_csv('data/FLIR_groups1and2_train.csv')\n",
    "# test = pd.read_csv('data/FLIR_groups1and2_test.csv')\n",
    "\n",
    "#Read in data\n",
    "\n",
    "data_path = 'data/FLIR_groups1and2_train.csv'\n",
    "\n",
    "read_data = pd.read_csv(data_path, skiprows = 2)\n",
    "training_data_set = read_data.iloc[:, 2:]\n",
    "\n",
    "#Seperate by Rounds\n",
    "\n",
    "round_1 = training_data_set.iloc[:, :27]\n",
    "round_2 = training_data_set.iloc[:, 28:55]\n",
    "round_3 = training_data_set.iloc[:, 56:83]\n",
    "round_4 = training_data_set.iloc[:, 84:111]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T_offset_1</th>\n",
       "      <th>Max1R13_1</th>\n",
       "      <th>Max1L13_1</th>\n",
       "      <th>aveAllR13_1</th>\n",
       "      <th>aveAllL13_1</th>\n",
       "      <th>T_RC_1</th>\n",
       "      <th>T_RC_Dry_1</th>\n",
       "      <th>T_RC_Wet_1</th>\n",
       "      <th>T_RC_Max_1</th>\n",
       "      <th>T_LC_1</th>\n",
       "      <th>...</th>\n",
       "      <th>T_OR_Max_4</th>\n",
       "      <th>Unnamed: 113</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>T_atm</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Unnamed: 120</th>\n",
       "      <th>aveOralM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.58</td>\n",
       "      <td>34.98</td>\n",
       "      <td>35.36</td>\n",
       "      <td>34.44</td>\n",
       "      <td>34.85</td>\n",
       "      <td>34.91</td>\n",
       "      <td>34.91</td>\n",
       "      <td>34.60</td>\n",
       "      <td>34.98</td>\n",
       "      <td>35.31</td>\n",
       "      <td>...</td>\n",
       "      <td>36.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>41-50</td>\n",
       "      <td>White</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.83</td>\n",
       "      <td>34.71</td>\n",
       "      <td>34.51</td>\n",
       "      <td>34.46</td>\n",
       "      <td>34.24</td>\n",
       "      <td>34.68</td>\n",
       "      <td>34.68</td>\n",
       "      <td>34.44</td>\n",
       "      <td>34.71</td>\n",
       "      <td>34.65</td>\n",
       "      <td>...</td>\n",
       "      <td>35.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>31-40</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.85</td>\n",
       "      <td>35.70</td>\n",
       "      <td>35.44</td>\n",
       "      <td>35.00</td>\n",
       "      <td>34.78</td>\n",
       "      <td>35.67</td>\n",
       "      <td>35.67</td>\n",
       "      <td>35.46</td>\n",
       "      <td>35.70</td>\n",
       "      <td>35.41</td>\n",
       "      <td>...</td>\n",
       "      <td>36.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>21-30</td>\n",
       "      <td>White</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>35.17</td>\n",
       "      <td>35.50</td>\n",
       "      <td>34.25</td>\n",
       "      <td>35.00</td>\n",
       "      <td>35.14</td>\n",
       "      <td>35.14</td>\n",
       "      <td>35.08</td>\n",
       "      <td>35.17</td>\n",
       "      <td>35.50</td>\n",
       "      <td>...</td>\n",
       "      <td>35.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.08</td>\n",
       "      <td>35.33</td>\n",
       "      <td>35.55</td>\n",
       "      <td>34.31</td>\n",
       "      <td>35.14</td>\n",
       "      <td>35.50</td>\n",
       "      <td>35.30</td>\n",
       "      <td>35.50</td>\n",
       "      <td>35.52</td>\n",
       "      <td>35.53</td>\n",
       "      <td>...</td>\n",
       "      <td>36.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>18-20</td>\n",
       "      <td>White</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>0.89</td>\n",
       "      <td>35.68</td>\n",
       "      <td>35.62</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.10</td>\n",
       "      <td>35.62</td>\n",
       "      <td>35.62</td>\n",
       "      <td>35.24</td>\n",
       "      <td>35.68</td>\n",
       "      <td>35.60</td>\n",
       "      <td>...</td>\n",
       "      <td>36.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-20</td>\n",
       "      <td>White</td>\n",
       "      <td>24.4</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>0.96</td>\n",
       "      <td>35.58</td>\n",
       "      <td>35.58</td>\n",
       "      <td>35.27</td>\n",
       "      <td>35.28</td>\n",
       "      <td>35.56</td>\n",
       "      <td>35.56</td>\n",
       "      <td>35.47</td>\n",
       "      <td>35.58</td>\n",
       "      <td>35.58</td>\n",
       "      <td>...</td>\n",
       "      <td>36.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>21-25</td>\n",
       "      <td>Asian</td>\n",
       "      <td>24.4</td>\n",
       "      <td>14.7</td>\n",
       "      <td>0.63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>0.91</td>\n",
       "      <td>36.82</td>\n",
       "      <td>36.47</td>\n",
       "      <td>36.21</td>\n",
       "      <td>36.15</td>\n",
       "      <td>36.81</td>\n",
       "      <td>36.80</td>\n",
       "      <td>36.75</td>\n",
       "      <td>36.82</td>\n",
       "      <td>36.45</td>\n",
       "      <td>...</td>\n",
       "      <td>37.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>21-25</td>\n",
       "      <td>Multiracial</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>1.10</td>\n",
       "      <td>36.98</td>\n",
       "      <td>36.96</td>\n",
       "      <td>36.50</td>\n",
       "      <td>36.29</td>\n",
       "      <td>36.94</td>\n",
       "      <td>36.94</td>\n",
       "      <td>36.85</td>\n",
       "      <td>36.98</td>\n",
       "      <td>36.98</td>\n",
       "      <td>...</td>\n",
       "      <td>37.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>18-20</td>\n",
       "      <td>White</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>1.07</td>\n",
       "      <td>37.10</td>\n",
       "      <td>37.19</td>\n",
       "      <td>36.86</td>\n",
       "      <td>37.03</td>\n",
       "      <td>37.35</td>\n",
       "      <td>37.08</td>\n",
       "      <td>37.35</td>\n",
       "      <td>37.40</td>\n",
       "      <td>37.31</td>\n",
       "      <td>...</td>\n",
       "      <td>37.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>18-20</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>710 rows Ã— 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     T_offset_1  Max1R13_1  Max1L13_1  aveAllR13_1  aveAllL13_1  T_RC_1  \\\n",
       "0          0.58      34.98      35.36        34.44        34.85   34.91   \n",
       "1          0.83      34.71      34.51        34.46        34.24   34.68   \n",
       "2          0.85      35.70      35.44        35.00        34.78   35.67   \n",
       "3          0.90      35.17      35.50        34.25        35.00   35.14   \n",
       "4          1.08      35.33      35.55        34.31        35.14   35.50   \n",
       "..          ...        ...        ...          ...          ...     ...   \n",
       "705        0.89      35.68      35.62        35.06        35.10   35.62   \n",
       "706        0.96      35.58      35.58        35.27        35.28   35.56   \n",
       "707        0.91      36.82      36.47        36.21        36.15   36.81   \n",
       "708        1.10      36.98      36.96        36.50        36.29   36.94   \n",
       "709        1.07      37.10      37.19        36.86        37.03   37.35   \n",
       "\n",
       "     T_RC_Dry_1  T_RC_Wet_1  T_RC_Max_1  T_LC_1  ...  T_OR_Max_4  \\\n",
       "0         34.91       34.60       34.98   35.31  ...       36.39   \n",
       "1         34.68       34.44       34.71   34.65  ...       35.84   \n",
       "2         35.67       35.46       35.70   35.41  ...       36.40   \n",
       "3         35.14       35.08       35.17   35.50  ...       35.08   \n",
       "4         35.30       35.50       35.52   35.53  ...       36.64   \n",
       "..          ...         ...         ...     ...  ...         ...   \n",
       "705       35.62       35.24       35.68   35.60  ...       36.72   \n",
       "706       35.56       35.47       35.58   35.58  ...       36.74   \n",
       "707       36.80       36.75       36.82   36.45  ...       37.32   \n",
       "708       36.94       36.85       36.98   36.98  ...       37.35   \n",
       "709       37.08       37.35       37.40   37.31  ...       37.32   \n",
       "\n",
       "     Unnamed: 113  Gender    Age                  Ethnicity  T_atm  Humidity  \\\n",
       "0             NaN    Male  41-50                      White   24.0      28.0   \n",
       "1             NaN  Female  31-40  Black or African-American   24.0      26.0   \n",
       "2             NaN  Female  21-30                      White   24.0      26.0   \n",
       "3             NaN  Female  21-30  Black or African-American   24.0      27.0   \n",
       "4             NaN    Male  18-20                      White   24.0      27.0   \n",
       "..            ...     ...    ...                        ...    ...       ...   \n",
       "705           NaN  Female  18-20                      White   24.4      13.5   \n",
       "706           NaN  Female  21-25                      Asian   24.4      14.7   \n",
       "707           NaN    Male  21-25                Multiracial   22.0      30.0   \n",
       "708           NaN    Male  18-20                      White   22.0      30.0   \n",
       "709           NaN    Male  18-20  Black or African-American   22.0      30.0   \n",
       "\n",
       "     Distance  Unnamed: 120  aveOralM  \n",
       "0        0.80           NaN     36.59  \n",
       "1        0.80           NaN     37.19  \n",
       "2        0.80           NaN     37.34  \n",
       "3        0.80           NaN     37.09  \n",
       "4        0.80           NaN     37.04  \n",
       "..        ...           ...       ...  \n",
       "705      0.60           NaN     36.89  \n",
       "706      0.63           NaN     37.14  \n",
       "707      0.60           NaN     37.79  \n",
       "708      0.60           NaN     38.14  \n",
       "709      0.60           NaN     38.39  \n",
       "\n",
       "[710 rows x 120 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_features = training_data_set[['Gender', 'Age', 'Ethnicity', 'T_atm', 'Humidity', 'Distance', 'aveOralM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to find empty rows \n",
    "def find_empty(round):\n",
    "    empty_rows = []\n",
    "    \n",
    "    for i, row in round.iterrows():\n",
    "        if row.isnull().all():\n",
    "            empty_rows.append(i)\n",
    "            \n",
    "    return empty_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Rows in Row 1:  [17, 78, 132, 171, 236, 292, 413, 483, 593, 616, 662]\n",
      "Empty Rows in Row 2:  [22, 106, 171, 292, 413, 479, 609, 626, 686]\n",
      "Empty Rows in Row 3:  [22, 294, 386, 489]\n",
      "Empty Rows in Row 4:  [20, 76, 107, 113, 128, 187, 221, 236, 252, 272, 294, 325, 359, 414, 469, 484, 485, 560, 573, 639, 695]\n"
     ]
    }
   ],
   "source": [
    "round_1_empty_rows = find_empty(round_1)\n",
    "round_2_empty_rows = find_empty(round_2)\n",
    "round_3_empty_rows = find_empty(round_3)\n",
    "round_4_empty_rows = find_empty(round_4)\n",
    "\n",
    "print(\"Empty Rows in Row 1: \", round_1_empty_rows)\n",
    "print(\"Empty Rows in Row 2: \", round_2_empty_rows)\n",
    "print(\"Empty Rows in Row 3: \", round_3_empty_rows)\n",
    "print(\"Empty Rows in Row 4: \", round_4_empty_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the means of every feature\n",
    "\n",
    "round_1_means = (round_1.mean()).values  #dtype is float64\n",
    "round_2_means = (round_2.mean()).values\n",
    "round_3_means = (round_3.mean()).values\n",
    "round_4_means = (round_4.mean()).values\n",
    "\n",
    "# print(round_1_means)\n",
    "# print(round_2_means)\n",
    "# print(round_3_means)\n",
    "# print(round_4_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to insert means into missing rows \n",
    "\n",
    "def insert_mean(round_num, empty_rows, mean):\n",
    "    rounds_final = round_num\n",
    "    \n",
    "    for i in empty_rows:\n",
    "        rounds_final.loc[i] = mean\n",
    "\n",
    "    return rounds_final  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_round_1 = insert_mean(round_1, round_1_empty_rows, round_1_means)\n",
    "final_round_2 = insert_mean(round_2, round_2_empty_rows, round_2_means)\n",
    "final_round_3 = insert_mean(round_3, round_3_empty_rows, round_3_means)\n",
    "final_round_4 = insert_mean(round_4, round_4_empty_rows, round_4_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "test_empty = find_empty(final_round_4)\n",
    "print(test_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(710, 27)\n",
      "(710, 27)\n",
      "(710, 27)\n",
      "(710, 27)\n"
     ]
    }
   ],
   "source": [
    "# final_round_1\n",
    "\n",
    "# print(final_round_1.shape)\n",
    "# print(final_round_2.shape)\n",
    "# print(final_round_3.shape)\n",
    "# print(final_round_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "print(final_round_1.iloc[0, :].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final dataframe by calculating mean across 4 tables for each cell\n",
    "# concatenated_df = pd.concat([final_round_1, final_round_2, final_round_3, final_round_4])\n",
    "# final_df_values = concatenated_df.groupby(concatenated_df.index).mean()\n",
    "# final_df_values = (final_round_1 + final_round_2 + final_round_3 + final_round_4) / 4\n",
    "\n",
    "# final_df_values\n",
    "\n",
    "#compute final dataset with means from the 4 rounds (NO extra features yet)\n",
    "\n",
    "final_dataset_means = pd.DataFrame()\n",
    "\n",
    "for i in range(final_round_1.iloc[0, :].size):\n",
    "    avg_values = (final_round_1.iloc[:, i] + final_round_2.iloc[:, i] + final_round_3.iloc[:, i] + final_round_4.iloc[:, i]) / 4\n",
    "    final_dataset_means = pd.concat([final_dataset, avg_values], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0        0        0        0        0        0        0        0  \\\n",
      "0    0.7025  35.0300  35.3775  34.4000  34.9175  34.9850  34.9850  34.7625   \n",
      "1    0.7800  34.5500  34.5200  33.9300  34.2250  34.7100  34.6325  34.6400   \n",
      "2    0.8625  35.6525  35.5175  34.2775  34.8000  35.6850  35.6675  35.6150   \n",
      "3    0.9300  35.2225  35.6125  34.3850  35.2475  35.2075  35.2000  35.1175   \n",
      "4    0.8950  35.5450  35.6650  34.9100  35.3675  35.6025  35.4750  35.5700   \n",
      "..      ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "705  0.9325  35.4800  35.5300  34.9000  34.9900  35.5650  35.5650  35.1350   \n",
      "706  0.8550  35.6550  35.5325  35.1925  35.2075  35.6125  35.6000  35.4850   \n",
      "707  0.9700  36.7325  36.4600  36.2225  36.1150  36.7175  36.7150  36.6400   \n",
      "708  1.0725  36.9450  37.0675  36.3825  36.4825  36.9250  36.9200  36.8200   \n",
      "709  1.0750  36.9825  35.6625  35.8875  35.3825  37.1000  36.9700  37.0350   \n",
      "\n",
      "           0        0  ...        0        0        0  Gender    Age  \\\n",
      "0    35.0325  35.3375  ...  35.6350  35.6525  35.6525    Male  41-50   \n",
      "1    34.7425  34.5600  ...  35.0925  35.1075  35.1075  Female  31-40   \n",
      "2    35.7175  35.5025  ...  35.8600  35.8850  35.8850  Female  21-30   \n",
      "3    35.2250  35.5950  ...  34.9650  34.9825  34.9825  Female  21-30   \n",
      "4    35.6400  35.6400  ...  35.5875  35.6175  35.6175    Male  18-20   \n",
      "..       ...      ...  ...      ...      ...      ...     ...    ...   \n",
      "705  35.6300  35.5325  ...  35.7600  35.8100  35.8100  Female  18-20   \n",
      "706  35.6550  35.5275  ...  36.1725  36.1950  36.1950  Female  21-25   \n",
      "707  36.7350  36.4350  ...  36.7825  36.8325  36.8325    Male  21-25   \n",
      "708  36.9475  37.0500  ...  36.7275  36.7425  36.7425    Male  18-20   \n",
      "709  37.1375  37.0250  ...  36.4875  36.5200  36.5200    Male  18-20   \n",
      "\n",
      "                     Ethnicity  T_atm  Humidity  Distance  aveOralM  \n",
      "0                        White   24.0      28.0      0.80     36.59  \n",
      "1    Black or African-American   24.0      26.0      0.80     37.19  \n",
      "2                        White   24.0      26.0      0.80     37.34  \n",
      "3    Black or African-American   24.0      27.0      0.80     37.09  \n",
      "4                        White   24.0      27.0      0.80     37.04  \n",
      "..                         ...    ...       ...       ...       ...  \n",
      "705                      White   24.4      13.5      0.60     36.89  \n",
      "706                      Asian   24.4      14.7      0.63     37.14  \n",
      "707                Multiracial   22.0      30.0      0.60     37.79  \n",
      "708                      White   22.0      30.0      0.60     38.14  \n",
      "709  Black or African-American   22.0      30.0      0.60     38.39  \n",
      "\n",
      "[710 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# final_dataset_means\n",
    "# print(final_dataset.shape)\n",
    "\n",
    "\n",
    "final_dataset = pd.concat([final_dataset_means, extra_features], axis=1)\n",
    "# final_dataset\n",
    "print(final_dataset)\n",
    "\n",
    "\n",
    "# final_data_test = find_empty(final_dataset)\n",
    "# print(final_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T_offset_1     float64\n",
       "Max1R13_1      float64\n",
       "Max1L13_1      float64\n",
       "aveAllR13_1    float64\n",
       "aveAllL13_1    float64\n",
       "                ...   \n",
       "Ethnicity       object\n",
       "T_atm          float64\n",
       "Humidity       float64\n",
       "Distance       float64\n",
       "aveOralM       float64\n",
       "Length: 115, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intial Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Relation Between Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data PreProcessing \n",
    "* Removing Outliers\n",
    "* Filling in missing data/balancing out the data\n",
    "* Feature Importance\n",
    "* Feature Extraction/Dimensionality Reduction\n",
    "* Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing data with means \n",
    "#calculate means and std of every feature for every round \n",
    "# normalize data\n",
    "\n",
    "#pandas describe\n",
    "#pandas fillna\n",
    "\n",
    "\n",
    "# 1. compute stats for each round (mean and std)\n",
    "# 2. if stats for each round are similar, fill in missing data with means\n",
    "# 3. concatenate in round order\n",
    "# 4. group by subject ID\n",
    "# 4.5. feature engineering\n",
    "# 5. shuffle wrt subject ID\n",
    "# 6. split k-fold into train and val (90% 10%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trivial Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean Output Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Regression (No Regularisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-Class Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Nearest Neighbours Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MSE Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-Class Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Long Short Term Memory Neural Network (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
